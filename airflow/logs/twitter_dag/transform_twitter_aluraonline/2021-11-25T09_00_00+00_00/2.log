[2021-11-29 23:40:21,615] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twitter_aluraonline 2021-11-25T09:00:00+00:00 [queued]>
[2021-11-29 23:40:21,623] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twitter_aluraonline 2021-11-25T09:00:00+00:00 [queued]>
[2021-11-29 23:40:21,623] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2021-11-29 23:40:21,630] {taskinstance.py:881} INFO - Starting attempt 2 of 2
[2021-11-29 23:40:21,630] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2021-11-29 23:40:21,643] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_aluraonline> on 2021-11-25T09:00:00+00:00
[2021-11-29 23:40:21,645] {standard_task_runner.py:54} INFO - Started process 3580 to run task
[2021-11-29 23:40:21,760] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'twitter_dag', 'transform_twitter_aluraonline', '2021-11-25T09:00:00+00:00', '--job_id', '19', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/twitter_dag.py', '--cfg_path', '/tmp/tmp01rcshut']
[2021-11-29 23:40:21,760] {standard_task_runner.py:78} INFO - Job 19: Subtask transform_twitter_aluraonline
[2021-11-29 23:40:21,798] {logging_mixin.py:112} INFO - Running <TaskInstance: twitter_dag.transform_twitter_aluraonline 2021-11-25T09:00:00+00:00 [running]> on host kauai-VirtualBox
[2021-11-29 23:40:21,899] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2021-11-29 23:40:21,900] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: /home/kauai/spark-3.1.2-bin-hadoop3.2/bin/spark-submit --master local --name twitter_transformation /home/kauai/spark/transformation.py --src /home/kauai/kauai/datapipeline/datalake/bronze/twitter_aluraonline/extract_date=2021-11-25 --dest /home/kauai/kauai/datapipeline/datalake/silver/twitter_aluraonline/ --process-date 2021-11-25
[2021-11-29 23:40:23,932] {spark_submit_hook.py:479} INFO - 21/11/29 23:40:23 WARN Utils: Your hostname, kauai-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
[2021-11-29 23:40:23,932] {spark_submit_hook.py:479} INFO - 21/11/29 23:40:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
[2021-11-29 23:40:23,942] {spark_submit_hook.py:479} INFO - WARNING: An illegal reflective access operation has occurred
[2021-11-29 23:40:23,942] {spark_submit_hook.py:479} INFO - WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)
[2021-11-29 23:40:23,942] {spark_submit_hook.py:479} INFO - WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
[2021-11-29 23:40:23,942] {spark_submit_hook.py:479} INFO - WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
[2021-11-29 23:40:23,942] {spark_submit_hook.py:479} INFO - WARNING: All illegal access operations will be denied in a future release
[2021-11-29 23:40:24,604] {spark_submit_hook.py:479} INFO - 21/11/29 23:40:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2021-11-29 23:40:25,139] {spark_submit_hook.py:479} INFO - python3: can't open file '/home/kauai/spark/transformation.py': [Errno 2] No such file or directory
[2021-11-29 23:40:25,172] {spark_submit_hook.py:479} INFO - log4j:WARN No appenders could be found for logger (org.apache.spark.util.ShutdownHookManager).
[2021-11-29 23:40:25,172] {spark_submit_hook.py:479} INFO - log4j:WARN Please initialize the log4j system properly.
[2021-11-29 23:40:25,172] {spark_submit_hook.py:479} INFO - log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
[2021-11-29 23:40:25,208] {taskinstance.py:1150} ERROR - Cannot execute: /home/kauai/spark-3.1.2-bin-hadoop3.2/bin/spark-submit --master local --name twitter_transformation /home/kauai/spark/transformation.py --src /home/kauai/kauai/datapipeline/datalake/bronze/twitter_aluraonline/extract_date=2021-11-25 --dest /home/kauai/kauai/datapipeline/datalake/silver/twitter_aluraonline/ --process-date 2021-11-25. Error code is: 2.
Traceback (most recent call last):
  File "/home/kauai/datapipeline/venv/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/kauai/datapipeline/venv/lib/python3.8/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/kauai/datapipeline/venv/lib/python3.8/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 403, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: /home/kauai/spark-3.1.2-bin-hadoop3.2/bin/spark-submit --master local --name twitter_transformation /home/kauai/spark/transformation.py --src /home/kauai/kauai/datapipeline/datalake/bronze/twitter_aluraonline/extract_date=2021-11-25 --dest /home/kauai/kauai/datapipeline/datalake/silver/twitter_aluraonline/ --process-date 2021-11-25. Error code is: 2.
[2021-11-29 23:40:25,209] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=twitter_dag, task_id=transform_twitter_aluraonline, execution_date=20211125T090000, start_date=20211130T024021, end_date=20211130T024025
[2021-11-29 23:40:26,584] {local_task_job.py:102} INFO - Task exited with return code 1
